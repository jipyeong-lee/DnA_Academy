{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=CC3D3D>\n",
    "\n",
    "# 잡플래닛 기업 리뷰 데이터 감성분석 </font>\n",
    "    \n",
    "다양한 토큰화, 임베딩 방법을 사용해서 RNN 성능을 높여주세요!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network\n",
    "\n",
    "- [Reading Data](#Reading-Data)\n",
    "- [Pre-processing Data](#Pre-processing-Data)\n",
    "- [Making Vocab & Setting Embedding](#Making-Vocab-&-Setting-Embedding)\n",
    "- [Spliting Validation Data & Making Data Iterator](#Spliting-Validation-Data-&-Making-Data-Iterator)\n",
    "    - [Sample Data](#Sample-Data)\n",
    "- [Modeling](#Modeling)\n",
    "    - [Checking feed-forward](#Checking-feed-forward)\n",
    "    - [Training](#Training)\n",
    "    - [bi-RNN](#bi-RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from torchtext import data\n",
    "from torchtext import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip uninstall transformers\n",
    "#pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "korean_stopwords = pd.read_csv('./korean_stopwords.txt', sep='\\n', header=None)\n",
    "korean_stopwords.columns = ['stopwords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kobert_transformers import get_tokenizer\n",
    "\n",
    "def tokenizer_kobert(text):\n",
    "    tokenizer = get_tokenizer()\n",
    "    result = tokenizer.tokenize(text)\n",
    "    for word in result:\n",
    "        if (word not in list(korean_stopwords['stopwords'])):\n",
    "            yield word\n",
    "# 너무 오래걸림\n",
    "tokenizer = get_tokenizer() # 바로 kobert로 tokenize하는 방법 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kiwipiepy import Kiwi, Option\n",
    "kiwi = Kiwi(num_workers=4, options=Option.LOAD_DEFAULT_DICTIONARY | Option.INTEGRATE_ALLOMORPH)\n",
    "kiwi.prepare()\n",
    "\n",
    "def tokenizer_kiwi(text):\n",
    "    result = kiwi.analyze(text)\n",
    "    for token, _, _, _ in result[0][0]:\n",
    "            yield f'{token}'\n",
    "# 성능이 생각보다 안나옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"beomi/KcELECTRA-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(batch_first = True, \n",
    "                  fix_length = 200, \n",
    "                  tokenize = tokenizer.tokenize, \n",
    "                  pad_first = True, \n",
    "                  pad_token ='[PAD]', \n",
    "                  unk_token ='[UNK]')\n",
    "\n",
    "LABEL = data.LabelField(dtype = torch.float)\n",
    "\n",
    "train_data = data.TabularDataset(path = 'train_data.csv', \n",
    "                    format = 'csv', \n",
    "                    fields = [(\"text\",TEXT),\n",
    "                            (\"label\",LABEL)],  \n",
    "                    skip_header=True)\n",
    "\n",
    "test_data = data.TabularDataset(path='test_data.csv', \n",
    "                    format='csv', \n",
    "                    fields=[(\"text\",TEXT),\n",
    "                            (\"label\",LABEL)],  \n",
    "                    skip_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Length : 50000\n",
      "Test Data Length : 15000\n"
     ]
    }
   ],
   "source": [
    "print(f'Train Data Length : {len(train_data.examples)}')\n",
    "print(f'Test Data Length : {len(test_data.examples)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Data Sample ----\n",
      "Input : \n",
      "절대 ##가지 ##마세요 몸 다 상 ##합니다 부서 절대 ##요 \n",
      "\n",
      "Label : \n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print('---- Data Sample ----')\n",
    "print('Input : ')\n",
    "print(' '.join(vars(train_data.examples[0])['text']),'\\n') \n",
    "print('Label : ')\n",
    "print(vars(train_data.examples[1])['label'])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PreProcessingText(input_sentence):\n",
    "    #input_sentence = input_sentence.lower() # 소문자화\n",
    "    input_sentence = re.sub('<[^>]*>', repl= ' ', string = input_sentence) # \"<br />\" 처리\n",
    "    input_sentence = re.sub('[!\"#$%&\\()*+,-./:;<=>?@[\\\\]^_`▁{|}~]', repl= ' ', string = input_sentence) # 특수문자 처리 (\"'\" 제외)\n",
    "    input_sentence = re.sub('\\s+', repl= ' ', string = input_sentence) # 연속된 띄어쓰기 처리\n",
    "    return input_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 간단한 Data Cleansing 작업\n",
    "for example in train_data.examples:\n",
    "    vars(example)['text'] = PreProcessingText(' '.join(vars(example)['text'])).split()\n",
    "    \n",
    "for example in test_data.examples:\n",
    "    vars(example)['text'] = PreProcessingText(' '.join(vars(example)['text'])).split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Vocab & Setting Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {'emb_type' : '', 'emb_dim' : 300}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build_vocab() : Text Data와 Label Data의 Vocab을 만듦\n",
    "TEXT.build_vocab(train_data,  \n",
    "                 min_freq = 2,   \n",
    "                 max_size = None, \n",
    "                 vectors = f\"glove.6B.{model_config['emb_dim']}d\") \n",
    "\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "model_config['vocab_size'] = len(TEXT.vocab) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다양한 임베딩 방법 \n",
    "\n",
    "## pre-trained vector list\n",
    "# charngram.100d\n",
    "# fasttext.en.300d\n",
    "# fasttext.simple.300d\n",
    "# glove.42B.300d\n",
    "# glove.840B.300d\n",
    "# glove.twitter.27B.25d\n",
    "# glove.twitter.27B.50d\n",
    "# glove.twitter.27B.100d\n",
    "# glove.twitter.27B.200d\n",
    "# glove.6B.50d\n",
    "# glove.6B.100d\n",
    "# glove.6B.200d\n",
    "# glove.6B.300d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size : 4777\n",
      "Vocab Examples : \n",
      "\t [UNK] 0\n",
      "\t [PAD] 1\n",
      "\t 이 2\n",
      "\t 회사 3\n",
      "\t 가 4\n",
      "\t 수 5\n",
      "\t 는 6\n",
      "\t 도 7\n",
      "\t 고 8\n",
      "\t 에 9\n",
      "---------------------------------\n",
      "Label Size : 2\n",
      "Lable Examples : \n",
      "\t 0 0\n",
      "\t 1 1\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary Info\n",
    "print(f'Vocab Size : {len(TEXT.vocab)}')\n",
    "\n",
    "print('Vocab Examples : ')\n",
    "for idx, (k, v) in enumerate(TEXT.vocab.stoi.items()):\n",
    "    if idx >= 10:  \n",
    "        break    \n",
    "    print('\\t', k, v)\n",
    "\n",
    "print('---------------------------------')\n",
    "\n",
    "# Label Info\n",
    "print(f'Label Size : {len(LABEL.vocab)}')\n",
    "\n",
    "print('Lable Examples : ')\n",
    "for idx, (k, v) in enumerate(LABEL.vocab.stoi.items()):\n",
    "    print('\\t', k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spliting Validation Data & Making Data Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = train_data.split(random_state = random.seed(0), split_ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config['batch_size'] = 128\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(datasets=(train_data, valid_data, test_data), \n",
    "                                                                           batch_size=model_config['batch_size'], device=device, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[torchtext.data.batch.Batch of size 128]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 128x200 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 128 (GPU 0)]\n",
      "tensor([[   1,    1,    1,  ...,    3,  163,   13],\n",
      "        [   1,    1,    1,  ...,  324,    2,  101],\n",
      "        [   1,    1,    1,  ...,   55,  705, 1394],\n",
      "        ...,\n",
      "        [   1,    1,    1,  ...,   47,   24,    3],\n",
      "        [   1,    1,    1,  ...,   38,  442,  389],\n",
      "        [   1,    1,    1,  ...,   36,    4,  303]], device='cuda:0')\n",
      "tensor([0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
      "        0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1.,\n",
      "        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 1.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Check batch data\n",
    "sample_for_check = next(iter(train_iterator))  \n",
    "print(sample_for_check)\n",
    "print(sample_for_check.text)\n",
    "print(sample_for_check.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "그냥 너무 힘 듬 ᅲᅲ 젊 음을 갈 아 서 성장 하는 회사 같 음\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(' '.join([TEXT.vocab.itos[int(x)] for x in sample_for_check.text[0,:] if x not in [0,1]]))\n",
    "print(LABEL.vocab.itos[int(sample_for_check.label[0])]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceClassification(nn.Module):\n",
    "    def __init__(self, **model_config):\n",
    "        super(SentenceClassification, self).__init__()\n",
    "\n",
    "        if model_config['emb_type'] == 'glove' or 'fasttext':   \n",
    "            self.emb = nn.Embedding(num_embeddings = model_config['vocab_size'],  \n",
    "                                    embedding_dim = model_config['emb_dim'],\n",
    "                                    _weight = TEXT.vocab.vectors)  \n",
    "        else:\n",
    "            self.emb = nn.Embedding(num_embeddings = model_config['vocab_size'],\n",
    "                                    embedding_dim = model_config['emb_dim'])\n",
    "        \n",
    "        self.bidirectional = model_config['bidirectional'] \n",
    "        self.num_direction = 2 if model_config['bidirectional'] else 1\n",
    "        self.model_type = model_config['model_type'] \n",
    "\n",
    "        self.RNN = nn.RNN (input_size = model_config['emb_dim'],  \n",
    "                           hidden_size = model_config['hidden_dim'],  \n",
    "                           dropout = model_config['dropout'],  \n",
    "                           bidirectional = model_config['bidirectional'], \n",
    "                           batch_first = model_config['batch_first'])  \n",
    "    \n",
    "        self.fc = nn.Linear(model_config['hidden_dim'] * self.num_direction, model_config['output_dim'])\n",
    "        \n",
    "        self.drop = nn.Dropout(model_config['dropout'])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        emb = self.emb(x) \n",
    "        output, hidden = self.RNN(emb) \n",
    "        \n",
    "        last_output = output[:,-1,:]\n",
    "\n",
    "        return self.fc(self.drop(last_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking feed-forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config.update(dict(batch_first = True,\n",
    "                         model_type = 'RNN',\n",
    "                         bidirectional = True,\n",
    "                         hidden_dim = 128,\n",
    "                         output_dim = 1,\n",
    "                         dropout = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceClassification(**model_config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = model.forward(sample_for_check.text).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCEWithLogitsLoss().to(device)\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() \n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_fn(predictions, sample_for_check.label)\n",
    "acc = binary_accuracy(predictions, sample_for_check.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0072, -0.0072, -0.0072, -0.0072, -0.0072, -0.0072, -0.0072, -0.0072,\n",
      "        -0.0072, -0.0072, -0.0072, -0.0072, -0.0072, -0.0072, -0.0072, -0.0072,\n",
      "        -0.0072, -0.0072, -0.0072, -0.0072, -0.0072, -0.0072, -0.0072, -0.0072,\n",
      "        -0.0072, -0.0072, -0.0072, -0.0072, -0.0072, -0.0072, -0.0072, -0.0072,\n",
      "        -0.0072, -0.0072, -0.0072, -0.0072, -0.0072, -0.0072, -0.0072, -0.0072,\n",
      "        -0.0072, -0.0072, -0.0072, -0.0072, -0.0072, -0.0072, -0.0072, -0.0072,\n",
      "        -0.0072, -0.0072, -0.0072, -0.0072, -0.0072, -0.0072, -0.0072, -0.0072,\n",
      "        -0.0072, -0.0072, -0.0072, -0.0072, -0.0072, -0.0072, -0.0072, -0.0072,\n",
      "        -0.0072, -0.0072, -0.0072, -0.0072, -0.0072, -0.0072, -0.0072, -0.0072,\n",
      "        -0.0072, -0.0072, -0.0072, -0.0072, -0.0072, -0.0072, -0.0072, -0.0072,\n",
      "        -0.0072, -0.0072, -0.0072, -0.0072, -0.0072, -0.0072, -0.0072, -0.0072,\n",
      "        -0.0072, -0.0072, -0.0072, -0.0072, -0.0072, -0.0072, -0.0072, -0.0072,\n",
      "        -0.0072, -0.0072, -0.0072, -0.0072, -0.0072, -0.0072, -0.0072, -0.0072,\n",
      "        -0.0072, -0.0072, -0.0072, -0.0072, -0.0072, -0.0072, -0.0072, -0.0072,\n",
      "        -0.0072, -0.0072, -0.0072, -0.0072, -0.0072, -0.0072, -0.0072, -0.0072,\n",
      "        -0.0072, -0.0072, -0.0072, -0.0072, -0.0072, -0.0072, -0.0072, -0.0072],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>) tensor(0.4453, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(predictions)\n",
    "print(loss, acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, loss_fn, idx_epoch, **model_params):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train() \n",
    "    batch_size = model_params['batch_size']\n",
    "\n",
    "    for idx, batch in enumerate(iterator):\n",
    "        \n",
    "        # Initializing\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward \n",
    "        predictions = model(batch.text).squeeze()\n",
    "        loss = loss_fn(predictions, batch.label)\n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        sys.stdout.write(\n",
    "                    \"\\r\" + f\"[Train] Epoch : {idx_epoch:^3}\"\\\n",
    "                    f\"[{(idx + 1) * batch_size} / {len(iterator) * batch_size} ({100. * (idx + 1) / len(iterator) :.4}%)]\"\\\n",
    "                    f\"  Loss: {loss.item():.4}\"\\\n",
    "                    f\"  Acc : {acc.item():.4}\")\n",
    "\n",
    "        # Backward \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update Epoch Performance\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss/len(iterator) , epoch_acc/len(iterator) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, loss_fn):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    # evaluation mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            loss = loss_fn(predictions, batch.label)\n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bi-RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config['model_type'] = 'RNN'\n",
    "model = SentenceClassification(**model_config).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn = nn.BCEWithLogitsLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Model name : bi-RNN_\n",
      "---------------------------------\n",
      "[Train] Epoch :  0 [40064 / 40064 (100.0%)]  Loss: 0.5731  Acc : 0.7188\n",
      "\t Saved at 0-epoch\n",
      "\t Epoch : 0 | Train Loss : 0.5432 | Train Acc : 0.7245\n",
      "\t Epoch : 0 | Valid Loss : 0.4954 | Valid Acc : 0.7712\n",
      "[Train] Epoch :  1 [40064 / 40064 (100.0%)]  Loss: 0.4229  Acc : 0.7891\n",
      "\t Saved at 1-epoch\n",
      "\t Epoch : 1 | Train Loss : 0.461 | Train Acc : 0.79\n",
      "\t Epoch : 1 | Valid Loss : 0.4612 | Valid Acc : 0.7893\n",
      "[Train] Epoch :  2 [40064 / 40064 (100.0%)]  Loss: 0.4356  Acc : 0.8281\n",
      "\t Epoch : 2 | Train Loss : 0.4115 | Train Acc : 0.8205\n",
      "\t Epoch : 2 | Valid Loss : 0.4854 | Valid Acc : 0.7696\n",
      "[Train] Epoch :  3 [40064 / 40064 (100.0%)]  Loss: 0.3979  Acc : 0.8359\n",
      "\t Epoch : 3 | Train Loss : 0.3788 | Train Acc : 0.8379\n",
      "\t Epoch : 3 | Valid Loss : 0.4854 | Valid Acc : 0.783\n",
      "[Train] Epoch :  4 [40064 / 40064 (100.0%)]  Loss: 0.3675  Acc : 0.8516\n",
      "\t Epoch : 4 | Train Loss : 0.3426 | Train Acc : 0.858\n",
      "\t Epoch : 4 | Valid Loss : 0.5039 | Valid Acc : 0.7697\n",
      "[Train] Epoch :  5 [40064 / 40064 (100.0%)]  Loss: 0.2852  Acc : 0.9141\n",
      "\t Epoch : 5 | Train Loss : 0.3228 | Train Acc : 0.8671\n",
      "\t Epoch : 5 | Valid Loss : 0.5299 | Valid Acc : 0.7668\n",
      "[Train] Epoch :  6 [40064 / 40064 (100.0%)]  Loss: 0.3103  Acc : 0.8906\n",
      "\t Epoch : 6 | Train Loss : 0.2875 | Train Acc : 0.8873\n",
      "\t Epoch : 6 | Valid Loss : 0.5426 | Valid Acc : 0.7662\n",
      "[Train] Epoch :  7 [40064 / 40064 (100.0%)]  Loss: 0.276  Acc : 0.89846\n",
      "\t Epoch : 7 | Train Loss : 0.2594 | Train Acc : 0.8997\n",
      "\t Epoch : 7 | Valid Loss : 0.5948 | Valid Acc : 0.7557\n",
      "[Train] Epoch :  8 [40064 / 40064 (100.0%)]  Loss: 0.2498  Acc : 0.9219\n",
      "\t Epoch : 8 | Train Loss : 0.2407 | Train Acc : 0.9064\n",
      "\t Epoch : 8 | Valid Loss : 0.6322 | Valid Acc : 0.7652\n",
      "[Train] Epoch :  9 [40064 / 40064 (100.0%)]  Loss: 0.2046  Acc : 0.9141\n",
      "\t Epoch : 9 | Train Loss : 0.2193 | Train Acc : 0.9169\n",
      "\t Epoch : 9 | Valid Loss : 0.6751 | Valid Acc : 0.7582\n"
     ]
    }
   ],
   "source": [
    "N_EPOCH = 10\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "model_name = f\"{'bi-' if model_config['bidirectional'] else ''}{model_config['model_type']}_{model_config['emb_type']}\"\n",
    "\n",
    "print('---------------------------------')\n",
    "print(f'Model name : {model_name}')\n",
    "print('---------------------------------')\n",
    "\n",
    "for epoch in range(N_EPOCH):\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, loss_fn, epoch, **model_config)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, loss_fn)\n",
    "    print('')\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), f'./{model_name}.pt')\n",
    "        print(f'\\t Saved at {epoch}-epoch')\n",
    "\n",
    "    print(f'\\t Epoch : {epoch} | Train Loss : {train_loss:.4} | Train Acc : {train_acc:.4}')\n",
    "    print(f'\\t Epoch : {epoch} | Valid Loss : {valid_loss:.4} | Valid Acc : {valid_acc:.4}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss : 0.4779 | Test Acc : 0.7807\n"
     ]
    }
   ],
   "source": [
    "# Test set\n",
    "model.load_state_dict(torch.load(f'./{model_name}.pt'))\n",
    "test_loss, test_acc = evaluate(model, test_iterator, loss_fn)\n",
    "print(f'Test Loss : {test_loss:.4} | Test Acc : {test_acc:.4}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
