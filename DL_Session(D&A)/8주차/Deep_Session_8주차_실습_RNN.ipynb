{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network\n",
    "\n",
    "- [Reading Data](#Reading-Data)\n",
    "- [Pre-processing Data](#Pre-processing-Data)\n",
    "- [Making Vocab & Setting Embedding](#Making-Vocab-&-Setting-Embedding)\n",
    "- [Spliting Validation Data & Making Data Iterator](#Spliting-Validation-Data-&-Making-Data-Iterator)\n",
    "    - [Sample Data](#Sample-Data)\n",
    "- [Modeling](#Modeling)\n",
    "    - [Checking feed-forward](#Checking-feed-forward)\n",
    "    - [Function Definition](#Function-Definition)\n",
    "    - [bi-RNN](#bi-RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 974,
     "status": "ok",
     "timestamp": 1593193218656,
     "user": {
      "displayName": "seongsu bang",
      "photoUrl": "",
      "userId": "06124410414614761416"
     },
     "user_tz": -540
    },
    "id": "aJGwgEKR_GPg"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext import data  # 텍스트에 대한 여러 추상화 기능을 제공하는 자연어 처리 라이브러리\n",
    "from torchtext import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M3O6ttcHKrIq"
   },
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 41518,
     "status": "ok",
     "timestamp": 1593193202383,
     "user": {
      "displayName": "seongsu bang",
      "photoUrl": "",
      "userId": "06124410414614761416"
     },
     "user_tz": -540
    },
    "id": "1oJjy5xBJXAx",
    "outputId": "54ba92fa-db22-4f17-f175-737ce1f1fe7f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading aclImdb_v1.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".data\\imdb\\aclImdb_v1.tar.gz: 100%|███████████████████████████████████████████████| 84.1M/84.1M [00:22<00:00, 3.72MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Data Setting\n",
    "TEXT = data.Field(batch_first = True,  # Batch-Size를 Data Shape Axis의 가장 앞으로 설정\n",
    "                  fix_length = 500,  # 문장의 길이 제한\n",
    "                  tokenize = str.split,  # tokenize를 설정하는 옵션, 기본값은 띄어쓰기 기반\n",
    "                  pad_first = True,  # 패딩을 앞에서 줄 것인지(fix_length 대비 짧은 문장의 경우)\n",
    "                  pad_token = '[PAD]',  # padding에 대한 특수 토큰\n",
    "                  unk_token = '[UNK]')  # dict에 없는 token 표현방법\n",
    "\n",
    "LABEL = data.LabelField(dtype=torch.float)  # 가져올 데이터에 대한 Type 설정\n",
    "\n",
    "train_data, test_data = datasets.IMDB.splits(text_field = TEXT, label_field = LABEL)  # datasets안의 IMDB 데이터로 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 998,
     "status": "ok",
     "timestamp": 1593193204453,
     "user": {
      "displayName": "seongsu bang",
      "photoUrl": "",
      "userId": "06124410414614761416"
     },
     "user_tz": -540
    },
    "id": "34DoliQ6mdob",
    "outputId": "4506a83c-9078-497b-ee63-e0819e53e963",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Length : 25000\n",
      "Test Data Length : 25000\n"
     ]
    }
   ],
   "source": [
    "# Data Length\n",
    "print(f'Train Data Length : {len(train_data.examples)}')  # 데이터의 개수를 확인\n",
    "print(f'Test Data Length : {len(test_data.examples)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1089,
     "status": "ok",
     "timestamp": 1593193205912,
     "user": {
      "displayName": "seongsu bang",
      "photoUrl": "",
      "userId": "06124410414614761416"
     },
     "user_tz": -540
    },
    "id": "ViUZ13AM_HFQ",
    "outputId": "1eb1bb98-2c37-46f4-b641-c48f65623cda",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Data Sample ----\n",
      "Input : \n",
      "Homelessness (or Houselessness as George Carlin stated) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. Most people think of the homeless as just a lost cause while worrying about things such as racism, the war on Iraq, pressuring kids to succeed, technology, the elections, inflation, or worrying if they'll be next to end up on the streets.<br /><br />But what if you were given a bet to live on the streets for a month without the luxuries you once had from a home, the entertainment sets, a bathroom, pictures on the wall, a computer, and everything you once treasure to see what it's like to be homeless? That is Goddard Bolt's lesson.<br /><br />Mel Brooks (who directs) who stars as Bolt plays a rich man who has everything in the world until deciding to make a bet with a sissy rival (Jeffery Tambor) to see if he can live in the streets for thirty days without the luxuries; if Bolt succeeds, he can do what he wants with a future project of making more buildings. The bet's on where Bolt is thrown on the street with a bracelet on his leg to monitor his every move where he can't step off the sidewalk. He's given the nickname Pepto by a vagrant after it's written on his forehead where Bolt meets other characters including a woman by the name of Molly (Lesley Ann Warren) an ex-dancer who got divorce before losing her home, and her pals Sailor (Howard Morris) and Fumes (Teddy Wilson) who are already used to the streets. They're survivors. Bolt isn't. He's not used to reaching mutual agreements like he once did when being rich where it's fight or flight, kill or be killed.<br /><br />While the love connection between Molly and Bolt wasn't necessary to plot, I found \"Life Stinks\" to be one of Mel Brooks' observant films where prior to being a comedy, it shows a tender side compared to his slapstick work such as Blazing Saddles, Young Frankenstein, or Spaceballs for the matter, to show what it's like having something valuable before losing it the next day or on the other hand making a stupid bet like all rich people do when they don't know what to do with their money. Maybe they should give it to the homeless instead of using it like Monopoly money.<br /><br />Or maybe this film will inspire you to help others. \n",
      "\n",
      "Label : \n",
      "pos\n"
     ]
    }
   ],
   "source": [
    "# Data Sample\n",
    "print('---- Data Sample ----')\n",
    "print('Input : ')\n",
    "print(' '.join(vars(train_data.examples[1])['text']),'\\n')  # vars() : 데이터의 값을 직접 확인\n",
    "print('Label : ')\n",
    "print(vars(train_data.examples[1])['label'])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pn4ddGIQLL_u"
   },
   "source": [
    "## Pre-processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 976,
     "status": "ok",
     "timestamp": 1593193212853,
     "user": {
      "displayName": "seongsu bang",
      "photoUrl": "",
      "userId": "06124410414614761416"
     },
     "user_tz": -540
    },
    "id": "gpIQqgb_G41G"
   },
   "outputs": [],
   "source": [
    "def PreProcessingText(input_sentence):\n",
    "    input_sentence = input_sentence.lower() # 소문자화\n",
    "    input_sentence = re.sub('<[^>]*>', repl= ' ', string = input_sentence) # \"<br />\" 처리\n",
    "    input_sentence = re.sub('[!\"#$%&\\()*+,-./:;<=>?@[\\\\]^_`{|}~]', repl= ' ', string = input_sentence) # 특수문자 처리 (\"'\" 제외)\n",
    "    input_sentence = re.sub('\\s+', repl= ' ', string = input_sentence) # 연속된 띄어쓰기 처리\n",
    "    if input_sentence:\n",
    "        return input_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6108,
     "status": "ok",
     "timestamp": 1593193227668,
     "user": {
      "displayName": "seongsu bang",
      "photoUrl": "",
      "userId": "06124410414614761416"
     },
     "user_tz": -540
    },
    "id": "5cIqIf34a2SR"
   },
   "outputs": [],
   "source": [
    "# 간단한 Data Cleansing 작업\n",
    "for example in train_data.examples:\n",
    "    vars(example)['text'] = PreProcessingText(' '.join(vars(example)['text'])).split()\n",
    "    \n",
    "for example in test_data.examples:\n",
    "    vars(example)['text'] = PreProcessingText(' '.join(vars(example)['text'])).split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iSucMR06LR8Y"
   },
   "source": [
    "## Making Vocab & Setting Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 901,
     "status": "ok",
     "timestamp": 1593193228570,
     "user": {
      "displayName": "seongsu bang",
      "photoUrl": "",
      "userId": "06124410414614761416"
     },
     "user_tz": -540
    },
    "id": "NBI0uTQUSbdt"
   },
   "outputs": [],
   "source": [
    "model_config = {'emb_type' : '', 'emb_dim' : 300}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2903,
     "status": "ok",
     "timestamp": 1593193947508,
     "user": {
      "displayName": "seongsu bang",
      "photoUrl": "",
      "userId": "06124410414614761416"
     },
     "user_tz": -540
    },
    "id": "8LGSnQIsAHcv"
   },
   "outputs": [],
   "source": [
    "# build_vocab() : Text Data와 Label Data의 Vocab을 만듦\n",
    "TEXT.build_vocab(train_data,  \n",
    "                 min_freq = 2,  # vocab에 해당하는 token에 최소한으로 등장하는 횟수 \n",
    "                 max_size = None,  # 전체 vocab size 자체에 제한\n",
    "                 vectors = f\"glove.6B.{model_config['emb_dim']}d\")  # pre-trained vector\n",
    "\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "model_config['vocab_size'] = len(TEXT.vocab) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pre-trained vector list\n",
    "# charngram.100d\n",
    "# fasttext.en.300d\n",
    "# fasttext.simple.300d\n",
    "# glove.42B.300d\n",
    "# glove.840B.300d\n",
    "# glove.twitter.27B.25d\n",
    "# glove.twitter.27B.50d\n",
    "# glove.twitter.27B.100d\n",
    "# glove.twitter.27B.200d\n",
    "# glove.6B.50d\n",
    "# glove.6B.100d\n",
    "# glove.6B.200d\n",
    "# glove.6B.300d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 972,
     "status": "ok",
     "timestamp": 1593193950818,
     "user": {
      "displayName": "seongsu bang",
      "photoUrl": "",
      "userId": "06124410414614761416"
     },
     "user_tz": -540
    },
    "id": "vt7ulofTn6U4",
    "outputId": "6a5b9a8c-6f08-4097-d30a-29d0ab8ac582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size : 51956\n",
      "Vocab Examples : \n",
      "\t [UNK] 0\n",
      "\t [PAD] 1\n",
      "\t the 2\n",
      "\t and 3\n",
      "\t a 4\n",
      "\t of 5\n",
      "\t to 6\n",
      "\t is 7\n",
      "\t in 8\n",
      "\t it 9\n",
      "---------------------------------\n",
      "Label Size : 2\n",
      "Lable Examples : \n",
      "\t neg 0\n",
      "\t pos 1\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary Info\n",
    "print(f'Vocab Size : {len(TEXT.vocab)}')\n",
    "\n",
    "print('Vocab Examples : ')\n",
    "for idx, (k, v) in enumerate(TEXT.vocab.stoi.items()):\n",
    "    if idx >= 10:  # 상위 10개 Vacab의 단어와 index 값을 가져옴\n",
    "        break    \n",
    "    print('\\t', k, v)\n",
    "\n",
    "print('---------------------------------')\n",
    "\n",
    "# Label Info\n",
    "print(f'Label Size : {len(LABEL.vocab)}')\n",
    "\n",
    "print('Lable Examples : ')\n",
    "for idx, (k, v) in enumerate(LABEL.vocab.stoi.items()):\n",
    "    print('\\t', k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zuSh9SlQLfGp"
   },
   "source": [
    "## Spliting Validation Data & Making Data Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1050,
     "status": "ok",
     "timestamp": 1593194358662,
     "user": {
      "displayName": "seongsu bang",
      "photoUrl": "",
      "userId": "06124410414614761416"
     },
     "user_tz": -540
    },
    "id": "CbH-Rha8___V"
   },
   "outputs": [],
   "source": [
    "# Spliting Valid set\n",
    "train_data, valid_data = train_data.split(random_state = random.seed(0), split_ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1216,
     "status": "ok",
     "timestamp": 1593194359983,
     "user": {
      "displayName": "seongsu bang",
      "photoUrl": "",
      "userId": "06124410414614761416"
     },
     "user_tz": -540
    },
    "id": "LKOfVNAwAJLU"
   },
   "outputs": [],
   "source": [
    "model_config['batch_size'] = 32\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(datasets=(train_data, valid_data, test_data), \n",
    "                                                                           batch_size=model_config['batch_size'], \n",
    "                                                                           device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 61452,
     "status": "ok",
     "timestamp": 1592906338611,
     "user": {
      "displayName": "seongsu bang",
      "photoUrl": "",
      "userId": "06124410414614761416"
     },
     "user_tz": -540
    },
    "id": "8OFTE-xAwJYb",
    "outputId": "c58f837f-f1f2-44a2-f04e-77fdb9d571ea",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[torchtext.data.batch.Batch of size 32]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 32x500 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 32 (GPU 0)]\n",
      "tensor([[   1,    1,    1,  ...,    7,  181, 2530],\n",
      "        [  51,    2, 1031,  ...,    2, 1110,  147],\n",
      "        [   1,    1,    1,  ...,   21,   57,   75],\n",
      "        ...,\n",
      "        [   1,    1,    1,  ...,    2,  950,  267],\n",
      "        [   1,    1,    1,  ...,  150,   15,    9],\n",
      "        [   1,    1,    1,  ..., 6062,    2,  265]], device='cuda:0')\n",
      "tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "        1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0.],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Check batch data\n",
    "sample_for_check = next(iter(train_iterator))  # 돌아가지 않을 경우 torchtext 버전이 0.3.1 버전이 맞는지 확인\n",
    "print(sample_for_check)\n",
    "print(sample_for_check.text)\n",
    "print(sample_for_check.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 61438,
     "status": "ok",
     "timestamp": 1592906338613,
     "user": {
      "displayName": "seongsu bang",
      "photoUrl": "",
      "userId": "06124410414614761416"
     },
     "user_tz": -540
    },
    "id": "NU1fESnXyR9E",
    "outputId": "6f981b3d-0a7c-401f-9fd8-939147741161",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this film is really bad it maybe harsh but it is it really is poor script every vampire cliché in the book is used and no sympathy is given at all to the origins of the main character i e ole dracula there have been some truly brilliant dracula vampire movies in the past but this doesn't even make it into the dire slot take a selection of people who seem to have dropped out of a teen slasher move add a dribble of dracula lore and mix in a heady tonic of religious surreal day dreaming and you get a confusing mess of a film dracula 2000 i really cannot find any good things to say about this movie as if it wasn't bad enough that it was made in the first place they seem to have made johnny lee miller effect an english accent whats the problem with that i hear you cry well he is english but he sounds like an american trying to do an english accent all in all you may as well say your money if you were thinking of buying it or rent it out watch it and discover for yourself why it's about as scary as the p s although la la is pretty frightening\n",
      "neg\n"
     ]
    }
   ],
   "source": [
    "# Check reverting data\n",
    "print(' '.join([TEXT.vocab.itos[int(x)] for x in sample_for_check.text[0,:] if x not in [0,1]]))\n",
    "print(LABEL.vocab.itos[int(sample_for_check.label[0])]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cZXWakE7MxU8"
   },
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CD2AK0NMAKr1"
   },
   "outputs": [],
   "source": [
    "class SentenceClassification(nn.Module):\n",
    "    def __init__(self, **model_config):\n",
    "        super(SentenceClassification, self).__init__()\n",
    "\n",
    "        if model_config['emb_type'] == 'glove' or 'fasttext':  # Vocab size * Embedding_dimension 행렬을 만들어 학습 \n",
    "            self.emb = nn.Embedding(num_embeddings = model_config['vocab_size'],  \n",
    "                                    embedding_dim = model_config['emb_dim'],  # 원하는 Embedding_dimension을 설정 \n",
    "                                    _weight = TEXT.vocab.vectors)  # Pre-Trained Vector를 Embedding 행렬의 Initial Value로 설정\n",
    "                                                                   # 이 옵션이 없는 경우 정규 분포에서 생성한 값을 Initial Value로 설정\n",
    "        else:\n",
    "            self.emb = nn.Embedding(num_embeddings = model_config['vocab_size'],\n",
    "                                    embedding_dim = model_config['emb_dim'])\n",
    "        \n",
    "        self.bidirectional = model_config['bidirectional']  # 두 개의 독립적인 RNN을 합친 것으로 정방향과 역방향 순서 정보를 모두 학습\n",
    "        self.num_direction = 2 if model_config['bidirectional'] else 1\n",
    "        self.model_type = model_config['model_type'] \n",
    "\n",
    "        self.RNN = nn.RNN (input_size = model_config['emb_dim'],  \n",
    "                           hidden_size = model_config['hidden_dim'],  \n",
    "                           dropout = model_config['dropout'],  \n",
    "                           bidirectional = model_config['bidirectional'], \n",
    "                           batch_first = model_config['batch_first'])  # 사용자 지정 Hyperparameter\n",
    "    \n",
    "        self.fc = nn.Linear(model_config['hidden_dim'] * self.num_direction, model_config['output_dim'])\n",
    "        \n",
    "        self.drop = nn.Dropout(model_config['dropout'])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x : (Batch_Size, Max_Seq_Length)\n",
    "        \n",
    "        emb = self.emb(x) \n",
    "        # emb : (Batch_Size, Max_Seq_Length, Emb_dim)\n",
    "\n",
    "        output, hidden = self.RNN(emb) \n",
    "        # output : (Batch_Size, Max_Seq_Length, Hidden_dim * num_direction) \n",
    "        # hidden : (num_direction, Batch_Size, Hidden_dim)\n",
    "        # hidden의 경우, batch_first 옵션이 안먹는 문제가 있음\n",
    "        \n",
    "        last_output = output[:,-1,:]\n",
    "        # last_output : (Batch_Size, Hidden_dim * num_direction)\n",
    "        \n",
    "        return self.fc(self.drop(last_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X_3DNQR3M30p"
   },
   "source": [
    "### Checking feed-forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r3VJ5BuHkm-4"
   },
   "outputs": [],
   "source": [
    "model_config.update(dict(batch_first = True,\n",
    "                         model_type = 'RNN',\n",
    "                         bidirectional = True,\n",
    "                         hidden_dim = 128,\n",
    "                         output_dim = 1,\n",
    "                         dropout = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aWYjr7KQZxXu"
   },
   "outputs": [],
   "source": [
    "model = SentenceClassification(**model_config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uhGNULElZuI_"
   },
   "outputs": [],
   "source": [
    "predictions = model.forward(sample_for_check.text).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wabzuaD1AKjF"
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.BCEWithLogitsLoss().to(device)\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() \n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HdOXGw9ze-lI"
   },
   "outputs": [],
   "source": [
    "loss = loss_fn(predictions, sample_for_check.label)\n",
    "acc = binary_accuracy(predictions, sample_for_check.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 61680,
     "status": "ok",
     "timestamp": 1592906338921,
     "user": {
      "displayName": "seongsu bang",
      "photoUrl": "",
      "userId": "06124410414614761416"
     },
     "user_tz": -540
    },
    "id": "s5OBri4AsPVy",
    "outputId": "397eee3e-3b7d-470e-98b8-9eb21bf62c45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.0928e-01, -1.7535e-02,  3.9715e-02, -1.1967e-01,  1.5811e-01,\n",
      "        -1.6251e-02, -8.0971e-02,  2.3031e-01,  1.8376e-01, -7.3657e-02,\n",
      "        -8.7040e-02,  6.1704e-02,  5.4594e-02, -4.0003e-02,  8.7493e-02,\n",
      "         5.0396e-05, -7.8464e-02, -1.8243e-01, -1.8300e-02, -5.4865e-02,\n",
      "         1.6881e-01, -1.1956e-01,  1.0026e-01, -1.1243e-01,  4.3017e-02,\n",
      "        -1.1112e-01, -9.2139e-02, -3.8387e-02,  8.8617e-02,  1.0152e-01,\n",
      "        -1.0894e-01,  3.5561e-02], device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "tensor(0.6895, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>) tensor(0.5000, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(predictions)\n",
    "print(loss, acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rswNBOAzoImC"
   },
   "source": [
    "### Function Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WCgs1fdcAKXF"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, loss_fn, idx_epoch, **model_params):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train() \n",
    "    batch_size = model_params['batch_size']\n",
    "\n",
    "    for idx, batch in enumerate(iterator):\n",
    "        \n",
    "        # Initializing\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward \n",
    "        predictions = model(batch.text).squeeze()\n",
    "        loss = loss_fn(predictions, batch.label)\n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        sys.stdout.write(\n",
    "                    \"\\r\" + f\"[Train] Epoch : {idx_epoch:^3}\"\\\n",
    "                    f\"[{(idx + 1) * batch_size} / {len(iterator) * batch_size} ({100. * (idx + 1) / len(iterator) :.4}%)]\"\\\n",
    "                    f\"  Loss: {loss.item():.4}\"\\\n",
    "                    f\"  Acc : {acc.item():.4}\")\n",
    "\n",
    "        # Backward \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update Epoch Performance\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss/len(iterator) , epoch_acc/len(iterator) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3zqJ1gkFARwp"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, loss_fn):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    # evaluation mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            loss = loss_fn(predictions, batch.label)\n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zIA_7QQzoLK1"
   },
   "source": [
    "### bi-RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n6GtVuVFUHqz"
   },
   "outputs": [],
   "source": [
    "model_config['model_type'] = 'RNN'\n",
    "model = SentenceClassification(**model_config).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn = nn.BCEWithLogitsLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'emb_type': '',\n",
       " 'emb_dim': 300,\n",
       " 'vocab_size': 51956,\n",
       " 'batch_size': 32,\n",
       " 'batch_first': True,\n",
       " 'model_type': 'RNN',\n",
       " 'bidirectional': True,\n",
       " 'hidden_dim': 128,\n",
       " 'output_dim': 1,\n",
       " 'dropout': 0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 364107,
     "status": "ok",
     "timestamp": 1592906641386,
     "user": {
      "displayName": "seongsu bang",
      "photoUrl": "",
      "userId": "06124410414614761416"
     },
     "user_tz": -540
    },
    "id": "uTvlbDLnUHq2",
    "outputId": "3f3f3a88-890f-4e46-df4d-4abb24cbdf92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Model name : bi-RNN_\n",
      "---------------------------------\n",
      "[Train] Epoch :  0 [20000 / 20000 (100.0%)]  Loss: 0.664  Acc : 0.53122\n",
      "\t Saved at 0-epoch\n",
      "\t Epoch : 0 | Train Loss : 0.647 | Train Acc : 0.6181\n",
      "\t Epoch : 0 | Valid Loss : 0.6119 | Valid Acc : 0.661\n",
      "[Train] Epoch :  1 [20000 / 20000 (100.0%)]  Loss: 0.6793  Acc : 0.6255\n",
      "\t Saved at 1-epoch\n",
      "\t Epoch : 1 | Train Loss : 0.5403 | Train Acc : 0.7298\n",
      "\t Epoch : 1 | Valid Loss : 0.5741 | Valid Acc : 0.7191\n",
      "[Train] Epoch :  2 [20000 / 20000 (100.0%)]  Loss: 0.5618  Acc : 0.7188\n",
      "\t Epoch : 2 | Train Loss : 0.524 | Train Acc : 0.7346\n",
      "\t Epoch : 2 | Valid Loss : 0.605 | Valid Acc : 0.6883\n",
      "[Train] Epoch :  3 [20000 / 20000 (100.0%)]  Loss: 0.4236  Acc : 0.8125\n",
      "\t Epoch : 3 | Train Loss : 0.4362 | Train Acc : 0.7941\n",
      "\t Epoch : 3 | Valid Loss : 0.5875 | Valid Acc : 0.7116\n",
      "[Train] Epoch :  4 [20000 / 20000 (100.0%)]  Loss: 0.4493  Acc : 0.8125\n",
      "\t Epoch : 4 | Train Loss : 0.3529 | Train Acc : 0.8502\n",
      "\t Epoch : 4 | Valid Loss : 0.6146 | Valid Acc : 0.7195\n"
     ]
    }
   ],
   "source": [
    "N_EPOCH = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "model_name = f\"{'bi-' if model_config['bidirectional'] else ''}{model_config['model_type']}_{model_config['emb_type']}\"\n",
    "\n",
    "print('---------------------------------')\n",
    "print(f'Model name : {model_name}')\n",
    "print('---------------------------------')\n",
    "\n",
    "for epoch in range(N_EPOCH):\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, loss_fn, epoch, **model_config)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, loss_fn)\n",
    "    print('')\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), f'./{model_name}.pt')\n",
    "        print(f'\\t Saved at {epoch}-epoch')\n",
    "\n",
    "    print(f'\\t Epoch : {epoch} | Train Loss : {train_loss:.4} | Train Acc : {train_acc:.4}')\n",
    "    print(f'\\t Epoch : {epoch} | Valid Loss : {valid_loss:.4} | Valid Acc : {valid_acc:.4}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 394038,
     "status": "ok",
     "timestamp": 1592906671343,
     "user": {
      "displayName": "seongsu bang",
      "photoUrl": "",
      "userId": "06124410414614761416"
     },
     "user_tz": -540
    },
    "id": "akDRWiykUHq5",
    "outputId": "da834104-d34f-4c5d-f009-78905305042a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss : 0.5625 | Test Acc : 0.7191\n"
     ]
    }
   ],
   "source": [
    "# Test set\n",
    "model.load_state_dict(torch.load(f'./{model_name}.pt'))\n",
    "test_loss, test_acc = evaluate(model, test_iterator, loss_fn)\n",
    "print(f'Test Loss : {test_loss:.4} | Test Acc : {test_acc:.4}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOye5Dt4Q3CTveRWxRMC3G8",
   "collapsed_sections": [],
   "name": "6-2_model_imdb_scratch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
