{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext import data\n",
    "from torchtext import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Seed 고정\n",
    "def torch_seed(random_seed=42):\n",
    "\n",
    "    torch.manual_seed(random_seed)\n",
    "\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed) # if use multi-GPU\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"beomi/KcELECTRA-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kiwipiepy import Kiwi, Option\n",
    "kiwi = Kiwi(num_workers=4, options=Option.LOAD_DEFAULT_DICTIONARY | Option.INTEGRATE_ALLOMORPH)\n",
    "kiwi.prepare()\n",
    "\n",
    "def tokenizer_kiwi(text):\n",
    "    result = kiwi.analyze(text)\n",
    "    for token, _, _, _ in result[0][0]:\n",
    "            yield f'{token}'\n",
    "# 성능이 생각보다 안나옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import ElectraTokenizer\n",
    "#tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(batch_first = True, fix_length = 200, \n",
    "                  tokenize=tokenizer_kiwi, pad_first=True, pad_token='[PAD]', unk_token='[UNK]')\n",
    "LABEL = data.LabelField(dtype=torch.float)\n",
    "\n",
    "train_data = data.TabularDataset(path='train_data.csv', \n",
    "                    format='csv', \n",
    "                    fields=[(\"text\",TEXT),\n",
    "                            (\"label\",LABEL)],  \n",
    "                    skip_header=True)\n",
    "\n",
    "test_data = data.TabularDataset(path='test_data.csv', \n",
    "                    format='csv', \n",
    "                    fields=[(\"text\",TEXT),\n",
    "                            (\"label\",LABEL)],  \n",
    "                    skip_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Length : 50000\n",
      "Test Data Length : 15000\n"
     ]
    }
   ],
   "source": [
    "print(f'Train Data Length : {len(train_data.examples)}')  # 데이터의 개수를 확인\n",
    "print(f'Test Data Length : {len(test_data.examples)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Data Sample ----\n",
      "Input : \n",
      "절대 가 지 말 시 어요 몸 다 상하 ᆸ니다 부서 절대 요 \n",
      "\n",
      "Label : \n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print('---- Data Sample ----')\n",
    "print('Input : ')\n",
    "print(' '.join(vars(train_data.examples[0])['text']),'\\n')  # vars() : 데이터의 값을 직접 확인\n",
    "print('Label : ')\n",
    "print(vars(train_data.examples[1])['label'])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PreProcessingText(input_sentence):\n",
    "    input_sentence = input_sentence.lower() # 소문자화\n",
    "    input_sentence = re.sub('<[^>]*>', repl= ' ', string = input_sentence) # \"<br />\" 처리\n",
    "    input_sentence = re.sub('[!\"#$%&\\()*+,-./:;<=>?@[\\\\]^_`{|}~]', repl= ' ', string = input_sentence) # 특수문자 처리 (\"'\" 제외)\n",
    "    input_sentence = re.sub('\\s+', repl= ' ', string = input_sentence) # 연속된 띄어쓰기 처리\n",
    "    return input_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 간단한 Data Cleansing 작업\n",
    "for example in train_data.examples:\n",
    "    vars(example)['text'] = PreProcessingText(' '.join(vars(example)['text'])).split()\n",
    "    \n",
    "for example in test_data.examples:\n",
    "    vars(example)['text'] = PreProcessingText(' '.join(vars(example)['text'])).split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Vocab & Setting Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {'emb_type' : '', 'emb_dim' : 300}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build_vocab() : Text Data와 Label Data의 Vocab을 만듦\n",
    "TEXT.build_vocab(train_data,  \n",
    "                 min_freq = 2,  # vocab에 해당하는 token에 최소한으로 등장하는 횟수 \n",
    "                 max_size = None,  # 전체 vocab size 자체에 제한\n",
    "                 vectors = f\"glove.6B.{model_config['emb_dim']}d\")  # pre-trained vector\n",
    "\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "model_config['vocab_size'] = len(TEXT.vocab) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size : 9136\n",
      "Vocab Examples : \n",
      "\t [UNK] 0\n",
      "\t [PAD] 1\n",
      "\t 하 2\n",
      "\t 이 3\n",
      "\t 는 4\n",
      "\t ᆫ 5\n",
      "\t 은 6\n",
      "\t 고 7\n",
      "\t 있 8\n",
      "\t 회사 9\n",
      "---------------------------------\n",
      "Label Size : 2\n",
      "Lable Examples : \n",
      "\t 0 0\n",
      "\t 1 1\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary Info\n",
    "print(f'Vocab Size : {len(TEXT.vocab)}')\n",
    "\n",
    "print('Vocab Examples : ')\n",
    "for idx, (k, v) in enumerate(TEXT.vocab.stoi.items()):\n",
    "    if idx >= 10:  # 상위 10개 Vacab의 단어와 index 값을 가져옴\n",
    "        break    \n",
    "    print('\\t', k, v)\n",
    "\n",
    "print('---------------------------------')\n",
    "\n",
    "# Label Info\n",
    "print(f'Label Size : {len(LABEL.vocab)}')\n",
    "\n",
    "print('Lable Examples : ')\n",
    "for idx, (k, v) in enumerate(LABEL.vocab.stoi.items()):\n",
    "    print('\\t', k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spliting Validation Data & Making Data Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = train_data.split(random_state = random.seed(42), split_ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config['batch_size'] = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(datasets=(train_data, valid_data, test_data), \n",
    "                                                                           batch_size=model_config['batch_size'], device=device, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 64x200 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "tensor([[   1,    1,    1,  ...,    3,    5,    9],\n",
      "        [   1,    1,    1,  ..., 1655,    8,   54],\n",
      "        [   1,    1,    1,  ...,   10,  123,  471],\n",
      "        ...,\n",
      "        [   1,    1,    1,  ...,  210,    5,  435],\n",
      "        [   1,    1,    1,  ...,   15,    6,    9],\n",
      "        [   1,    1,    1,  ...,   34,    4,   26]], device='cuda:0')\n",
      "tensor([1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0.,\n",
      "        0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "        0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 1., 0., 1., 0., 1., 0.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Check batch data\n",
    "sample_for_check = next(iter(train_iterator))  # 돌아가지 않을 경우 torchtext 버전이 0.3.1 버전이 맞는지 확인\n",
    "print(sample_for_check)\n",
    "print(sample_for_check.text)\n",
    "print(sample_for_check.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사람 들 이 잘 모르 지만 자기 도 모르 게 대한민국 의 많 은 직장인 들 이 이미 이용 중 이 ᆫ 회사\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(' '.join([TEXT.vocab.itos[int(x)] for x in sample_for_check.text[0,:] if x not in [0,1]]))\n",
    "print(LABEL.vocab.itos[int(sample_for_check.label[0])]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE HERE ... \n",
    "class SentenceClassification(nn.Module):\n",
    "    def __init__(self, **model_config):\n",
    "        super(SentenceClassification, self).__init__()\n",
    "\n",
    "        if model_config['emb_type'] == 'glove' or 'fasttext':  # Vocab size * Embedding_dimension 행렬을 만들어 학습 \n",
    "            self.emb = nn.Embedding(num_embeddings = model_config['vocab_size'],  \n",
    "                                    embedding_dim = model_config['emb_dim'],  # 원하는 Embedding_dimension을 설정 \n",
    "                                    _weight = TEXT.vocab.vectors)  # Pre-Trained Vector를 Embedding 행렬의 Initial Value로 설정\n",
    "                                                                   # 이 옵션이 없는 경우 정규 분포에서 생성한 값을 Initial Value로 설정\n",
    "        else:\n",
    "            self.emb = nn.Embedding(num_embeddings = model_config['vocab_size'],\n",
    "                                    embedding_dim = model_config['emb_dim'])\n",
    "        \n",
    "        self.bidirectional = model_config['bidirectional']  # 두 개의 독립적인 RNN을 합친 것으로 정방향과 역방향 순서 정보를 모두 학습\n",
    "        self.num_direction = 2 if model_config['bidirectional'] else 1\n",
    "        self.model_type = model_config['model_type'] \n",
    "        \n",
    "        if model_config['model_type'] == 'RNN':\n",
    "            self.RNN = nn.RNN (input_size = model_config['emb_dim'],  \n",
    "                           hidden_size = model_config['hidden_dim'],  \n",
    "                           dropout = model_config['dropout'],  \n",
    "                           bidirectional = model_config['bidirectional'], \n",
    "                           batch_first = model_config['batch_first'],\n",
    "                           num_layers = model_config['num_layers'])  # 사용자 지정 Hyperparameter\n",
    "            \n",
    "        elif model_config['model_type'] == 'LSTM':\n",
    "            self.RNN = nn.LSTM (input_size = model_config['emb_dim'],  \n",
    "                           hidden_size = model_config['hidden_dim'],  \n",
    "                           dropout = model_config['dropout'],  \n",
    "                           bidirectional = model_config['bidirectional'], \n",
    "                           batch_first = model_config['batch_first'],\n",
    "                           num_layers = model_config['num_layers'])  # 사용자 지정 Hyperparameter\n",
    "            \n",
    "        elif model_config['model_type'] == 'GRU':\n",
    "            self.RNN = nn.GRU (input_size = model_config['emb_dim'],  \n",
    "                           hidden_size = model_config['hidden_dim'],  \n",
    "                           dropout = model_config['dropout'],  \n",
    "                           bidirectional = model_config['bidirectional'], \n",
    "                           batch_first = model_config['batch_first'],\n",
    "                           num_layers = model_config['num_layers'])  # 사용자 지정 Hyperparameter\n",
    "        \n",
    "    \n",
    "        self.fc = nn.Linear(model_config['hidden_dim'] * self.num_direction, model_config['output_dim'])\n",
    "        \n",
    "        self.drop = nn.Dropout(model_config['dropout'])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x : (Batch_Size, Max_Seq_Length)\n",
    "        \n",
    "        emb = self.emb(x) \n",
    "        # emb : (Batch_Size, Max_Seq_Length, Emb_dim)\n",
    "\n",
    "        output, hidden = self.RNN(emb)\n",
    "        # output : (Batch_Size, Max_Seq_Length, Hidden_dim * num_direction) \n",
    "        # hidden : (num_direction, Batch_Size, Hidden_dim)\n",
    "        # hidden의 경우, batch_first 옵션이 안먹는 문제가 있음\n",
    "        \n",
    "        last_output = output[:,-1,:]\n",
    "        # last_output : (Batch_Size, Hidden_dim * num_direction)\n",
    "        \n",
    "        return self.fc(self.drop(last_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config.update(dict(batch_first = True,\n",
    "                         model_type = 'RNN',\n",
    "                         bidirectional = True,\n",
    "                         hidden_dim = 128,\n",
    "                         output_dim = 1,\n",
    "                         dropout = 0,\n",
    "                         num_layers = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceClassification(**model_config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.forward(sample_for_check.text).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCEWithLogitsLoss().to(device)\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() \n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_fn(predictions, sample_for_check.label)\n",
    "acc = binary_accuracy(predictions, sample_for_check.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0238, 0.0238, 0.0238, 0.0238, 0.0238, 0.0238, 0.0238, 0.0238, 0.0238,\n",
      "        0.0238, 0.0238, 0.0238, 0.0238, 0.0238, 0.0238, 0.0238, 0.0238, 0.0238,\n",
      "        0.0238, 0.0238, 0.0238, 0.0238, 0.0238, 0.0238, 0.0238, 0.0238, 0.0238,\n",
      "        0.0238, 0.0238, 0.0238, 0.0238, 0.0238, 0.0238, 0.0238, 0.0238, 0.0238,\n",
      "        0.0238, 0.0238, 0.0238, 0.0238, 0.0238, 0.0238, 0.0238, 0.0238, 0.0238,\n",
      "        0.0238, 0.0238, 0.0238, 0.0238, 0.0238, 0.0238, 0.0238, 0.0238, 0.0238,\n",
      "        0.0238, 0.0238, 0.0238, 0.0238, 0.0238, 0.0238, 0.0238, 0.0238, 0.0238,\n",
      "        0.0238], device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "tensor(0.6932, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>) tensor(0.5000, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(predictions)\n",
    "print(loss, acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, loss_fn, idx_epoch, **model_params):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train() \n",
    "    batch_size = model_params['batch_size']\n",
    "\n",
    "    for idx, batch in enumerate(iterator):\n",
    "        \n",
    "        # Initializing\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward \n",
    "        predictions = model(batch.text).squeeze()\n",
    "        loss = loss_fn(predictions, batch.label)\n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        sys.stdout.write(\n",
    "                    \"\\r\" + f\"[Train] Epoch : {idx_epoch:^3}\"\\\n",
    "                    f\"[{(idx + 1) * batch_size} / {len(iterator) * batch_size} ({100. * (idx + 1) / len(iterator) :.4}%)]\"\\\n",
    "                    f\"  Loss: {loss.item():.4}\"\\\n",
    "                    f\"  Acc : {acc.item():.4}\")\n",
    "\n",
    "        # Backward \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update Epoch Performance\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss/len(iterator) , epoch_acc/len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, loss_fn):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    # evaluation mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            loss = loss_fn(predictions, batch.label)\n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bi-RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config['model_type'] = 'RNN'\n",
    "model = SentenceClassification(**model_config).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn = nn.BCEWithLogitsLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Model name : bi-RNN_\n",
      "---------------------------------\n",
      "[Train] Epoch :  0 [40000 / 40000 (100.0%)]  Loss: 0.5392  Acc : 0.7588\n",
      "\t Saved at 0-epoch\n",
      "\t Epoch : 0 | Train Loss : 0.5322 | Train Acc : 0.7361\n",
      "\t Epoch : 0 | Valid Loss : 0.4961 | Valid Acc : 0.7688\n",
      "[Train] Epoch :  1 [40000 / 40000 (100.0%)]  Loss: 0.4884  Acc : 0.7656\n",
      "\t Saved at 1-epoch\n",
      "\t Epoch : 1 | Train Loss : 0.4415 | Train Acc : 0.8044\n",
      "\t Epoch : 1 | Valid Loss : 0.4858 | Valid Acc : 0.7735\n",
      "[Train] Epoch :  2 [40000 / 40000 (100.0%)]  Loss: 0.655  Acc : 0.65626\n",
      "\t Saved at 2-epoch\n",
      "\t Epoch : 2 | Train Loss : 0.3849 | Train Acc : 0.8365\n",
      "\t Epoch : 2 | Valid Loss : 0.4854 | Valid Acc : 0.7836\n",
      "[Train] Epoch :  3 [40000 / 40000 (100.0%)]  Loss: 0.2918  Acc : 0.8756\n",
      "\t Epoch : 3 | Train Loss : 0.341 | Train Acc : 0.8608\n",
      "\t Epoch : 3 | Valid Loss : 0.5108 | Valid Acc : 0.7822\n",
      "[Train] Epoch :  4 [40000 / 40000 (100.0%)]  Loss: 0.4528  Acc : 0.7969\n",
      "\t Epoch : 4 | Train Loss : 0.3154 | Train Acc : 0.8713\n",
      "\t Epoch : 4 | Valid Loss : 0.5239 | Valid Acc : 0.7712\n"
     ]
    }
   ],
   "source": [
    "N_EPOCH = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "model_name = f\"{'bi-' if model_config['bidirectional'] else ''}{model_config['model_type']}_{model_config['emb_type']}\"\n",
    "\n",
    "print('---------------------------------')\n",
    "print(f'Model name : {model_name}')\n",
    "print('---------------------------------')\n",
    "\n",
    "for epoch in range(N_EPOCH):\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, loss_fn, epoch, **model_config)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, loss_fn)\n",
    "    print('')\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), f'./{model_name}.pt')\n",
    "        print(f'\\t Saved at {epoch}-epoch')\n",
    "\n",
    "    print(f'\\t Epoch : {epoch} | Train Loss : {train_loss:.4} | Train Acc : {train_acc:.4}')\n",
    "    print(f'\\t Epoch : {epoch} | Valid Loss : {valid_loss:.4} | Valid Acc : {valid_acc:.4}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss : 0.486 | Test Acc : 0.7836\n"
     ]
    }
   ],
   "source": [
    "# Test set\n",
    "model.load_state_dict(torch.load(f'./{model_name}.pt'))\n",
    "test_loss, test_acc = evaluate(model, test_iterator, loss_fn)\n",
    "print(f'Test Loss : {test_loss:.4} | Test Acc : {test_acc:.4}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config['model_type'] = 'LSTM'\n",
    "model = SentenceClassification(**model_config).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn = nn.BCEWithLogitsLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Model name : bi-LSTM_\n",
      "---------------------------------\n",
      "[Train] Epoch :  0 [40000 / 40000 (100.0%)]  Loss: 0.4485  Acc : 0.8125\n",
      "\t Saved at 0-epoch\n",
      "\t Epoch : 0 | Train Loss : 0.4904 | Train Acc : 0.7643\n",
      "\t Epoch : 0 | Valid Loss : 0.4668 | Valid Acc : 0.7949\n",
      "[Train] Epoch :  1 [40000 / 40000 (100.0%)]  Loss: 0.3497  Acc : 0.8438\n",
      "\t Saved at 1-epoch\n",
      "\t Epoch : 1 | Train Loss : 0.3954 | Train Acc : 0.8258\n",
      "\t Epoch : 1 | Valid Loss : 0.4596 | Valid Acc : 0.7896\n",
      "[Train] Epoch :  2 [40000 / 40000 (100.0%)]  Loss: 0.3963  Acc : 0.8125\n",
      "\t Saved at 2-epoch\n",
      "\t Epoch : 2 | Train Loss : 0.3537 | Train Acc : 0.8465\n",
      "\t Epoch : 2 | Valid Loss : 0.4553 | Valid Acc : 0.7974\n",
      "[Train] Epoch :  3 [40000 / 40000 (100.0%)]  Loss: 0.3596  Acc : 0.8281\n",
      "\t Epoch : 3 | Train Loss : 0.3182 | Train Acc : 0.8647\n",
      "\t Epoch : 3 | Valid Loss : 0.4742 | Valid Acc : 0.7992\n",
      "[Train] Epoch :  4 [40000 / 40000 (100.0%)]  Loss: 0.3231  Acc : 0.8594\n",
      "\t Epoch : 4 | Train Loss : 0.2858 | Train Acc : 0.8793\n",
      "\t Epoch : 4 | Valid Loss : 0.5173 | Valid Acc : 0.7945\n"
     ]
    }
   ],
   "source": [
    "N_EPOCH = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "model_name = f\"{'bi-' if model_config['bidirectional'] else ''}{model_config['model_type']}_{model_config['emb_type']}\"\n",
    "\n",
    "print('---------------------------------')\n",
    "print(f'Model name : {model_name}')\n",
    "print('---------------------------------')\n",
    "\n",
    "for epoch in range(N_EPOCH):\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, loss_fn, epoch, **model_config)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, loss_fn)\n",
    "    print('')\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), f'./{model_name}.pt')\n",
    "        print(f'\\t Saved at {epoch}-epoch')\n",
    "\n",
    "    print(f'\\t Epoch : {epoch} | Train Loss : {train_loss:.4} | Train Acc : {train_acc:.4}')\n",
    "    print(f'\\t Epoch : {epoch} | Valid Loss : {valid_loss:.4} | Valid Acc : {valid_acc:.4}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss : 0.4519 | Test Acc : 0.802\n"
     ]
    }
   ],
   "source": [
    "# Test set\n",
    "model.load_state_dict(torch.load(f'./{model_name}.pt'))\n",
    "test_loss, test_acc = evaluate(model, test_iterator, loss_fn)\n",
    "print(f'Test Loss : {test_loss:.4} | Test Acc : {test_acc:.4}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config['model_type'] = 'GRU'\n",
    "model = SentenceClassification(**model_config).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn = nn.BCEWithLogitsLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Model name : bi-GRU_\n",
      "---------------------------------\n",
      "[Train] Epoch :  0 [40000 / 40000 (100.0%)]  Loss: 0.4304  Acc : 0.7969\n",
      "\t Saved at 0-epoch\n",
      "\t Epoch : 0 | Train Loss : 0.4789 | Train Acc : 0.7718\n",
      "\t Epoch : 0 | Valid Loss : 0.4318 | Valid Acc : 0.804\n",
      "[Train] Epoch :  1 [40000 / 40000 (100.0%)]  Loss: 0.5275  Acc : 0.7812\n",
      "\t Epoch : 1 | Train Loss : 0.3835 | Train Acc : 0.8341\n",
      "\t Epoch : 1 | Valid Loss : 0.439 | Valid Acc : 0.8025\n",
      "[Train] Epoch :  2 [40000 / 40000 (100.0%)]  Loss: 0.3683  Acc : 0.7812\n",
      "\t Epoch : 2 | Train Loss : 0.3361 | Train Acc : 0.8559\n",
      "\t Epoch : 2 | Valid Loss : 0.4563 | Valid Acc : 0.8021\n",
      "[Train] Epoch :  3 [40000 / 40000 (100.0%)]  Loss: 0.2106  Acc : 0.9375\n",
      "\t Epoch : 3 | Train Loss : 0.2974 | Train Acc : 0.8755\n",
      "\t Epoch : 3 | Valid Loss : 0.4627 | Valid Acc : 0.8008\n",
      "[Train] Epoch :  4 [40000 / 40000 (100.0%)]  Loss: 0.227  Acc : 0.93752\n",
      "\t Epoch : 4 | Train Loss : 0.264 | Train Acc : 0.8918\n",
      "\t Epoch : 4 | Valid Loss : 0.556 | Valid Acc : 0.7907\n"
     ]
    }
   ],
   "source": [
    "N_EPOCH = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "model_name = f\"{'bi-' if model_config['bidirectional'] else ''}{model_config['model_type']}_{model_config['emb_type']}\"\n",
    "\n",
    "print('---------------------------------')\n",
    "print(f'Model name : {model_name}')\n",
    "print('---------------------------------')\n",
    "\n",
    "for epoch in range(N_EPOCH):\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, loss_fn, epoch, **model_config)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, loss_fn)\n",
    "    print('')\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), f'./{model_name}.pt')\n",
    "        print(f'\\t Saved at {epoch}-epoch')\n",
    "\n",
    "    print(f'\\t Epoch : {epoch} | Train Loss : {train_loss:.4} | Train Acc : {train_acc:.4}')\n",
    "    print(f'\\t Epoch : {epoch} | Valid Loss : {valid_loss:.4} | Valid Acc : {valid_acc:.4}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss : 0.4386 | Test Acc : 0.8028\n"
     ]
    }
   ],
   "source": [
    "# Test set\n",
    "model.load_state_dict(torch.load(f'./{model_name}.pt'))\n",
    "test_loss, test_acc = evaluate(model, test_iterator, loss_fn)\n",
    "print(f'Test Loss : {test_loss:.4} | Test Acc : {test_acc:.4}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
