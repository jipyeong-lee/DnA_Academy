{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 과제) Cifar10 data를 사용한 모델 성능 향상시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "C:\\Users\\82105\\anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "learning_rate = 0.01\n",
    "epochs=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'> **데이터 정규화**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/CIFAR_10\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a5e5583109346f08d80f283ef61f00f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/CIFAR_10\\cifar-10-python.tar.gz to ./data/CIFAR_10\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "standardization = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4913, 0.4821, 0.4465], std = [0.2470, 0.2434, 0.2615])\n",
    "\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root = './data/CIFAR_10', train = True, download = True, transform = standardization)\n",
    "test_dataset = datasets.CIFAR10(root = './data/CIFAR_10', train = False, download = True, transform = standardization)\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle = True, drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset, batch_size = batch_size, shuffle = False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "batch_train, batch_labels = next(iter(train_loader))\n",
    "batch_test, batch_test_labels = next(iter(test_loader))\n",
    "print(batch_train.shape)\n",
    "print(batch_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Basic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>**Enhance Model**\n",
    "\n",
    "    \n",
    "    1. BatchNormalization : BatchNorm1d  \n",
    "\n",
    "    2. Activation Function : ReLU      \n",
    "\n",
    "    3. Initialization : He Initialization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cifar10_dnn(\n",
       "  (layer1): Sequential(\n",
       "    (0): Linear(in_features=3072, out_features=1024, bias=True)\n",
       "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=32, bias=True)\n",
       "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (layer5): Sequential(\n",
       "    (0): Linear(in_features=32, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class cifar10_dnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(cifar10_dnn, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(3 * 32 * 32, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Linear(512, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Linear(128, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Linear(32, 10),\n",
    "        )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = x.view(batch_size, -1)\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        return out\n",
    "\n",
    "            \n",
    "model = cifar10_dnn()\n",
    "\n",
    "def weight_init(m):                      \n",
    "    if isinstance(m, nn.Linear):                   \n",
    "        nn.init.kaiming_uniform_(m.weight.data)\n",
    "\n",
    "model.apply(weight_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'> **모델 테스트**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 10])\n"
     ]
    }
   ],
   "source": [
    "temp_images = batch_train\n",
    "test_prediction = model(temp_images)\n",
    "print(temp_images.shape)\n",
    "print(test_prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cifar10_dnn(\n",
       "  (layer1): Sequential(\n",
       "    (0): Linear(in_features=3072, out_features=1024, bias=True)\n",
       "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=32, bias=True)\n",
       "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (layer5): Sequential(\n",
       "    (0): Linear(in_features=32, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)  \n",
    "loss_func = nn.CrossEntropyLoss()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_arrs = []\n",
    "\n",
    "def fit(model, train_loader, epochs, optimizer, loss_func):\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        avg_loss = 0\n",
    "        total_batch = len(train_dataset) // batch_size\n",
    "        \n",
    "        for num, (images, labels) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = loss_func(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            avg_loss += loss / total_batch\n",
    "            loss_arrs.append(avg_loss)\n",
    "        print(f'[Epoch: {epoch+1}/{epochs}]  loss={avg_loss:.4f},  time={time.time()-start:.2f}')\n",
    "    print('Learning Finished !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 1/10]  loss=1.7162,  time=28.82\n",
      "[Epoch: 2/10]  loss=1.4622,  time=17.86\n",
      "[Epoch: 3/10]  loss=1.3469,  time=19.82\n",
      "[Epoch: 4/10]  loss=1.2551,  time=20.23\n",
      "[Epoch: 5/10]  loss=1.1763,  time=20.56\n",
      "[Epoch: 6/10]  loss=1.1097,  time=21.23\n",
      "[Epoch: 7/10]  loss=1.0513,  time=23.00\n",
      "[Epoch: 8/10]  loss=0.9750,  time=21.04\n",
      "[Epoch: 9/10]  loss=0.9153,  time=20.42\n",
      "[Epoch: 10/10]  loss=0.8508,  time=23.28\n",
      "Learning Finished !\n"
     ]
    }
   ],
   "source": [
    "fit(model, train_loader, epochs, optimizer, loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, epochs):\n",
    "    start = time.time()\n",
    "    test_acc = 0\n",
    "    \n",
    "    for num, (images, labels) in enumerate(test_loader):\n",
    "        test_images = images.to(device)\n",
    "        test_labels = labels.to(device)\n",
    "        \n",
    "        test_outputs = model(test_images)\n",
    "        test_batch_acc = ((test_outputs.argmax(dim=1) == test_labels).float().mean()) # acc = 맞춘 개수 / 배치사이즈\n",
    "        test_acc += test_batch_acc / len(test_loader) # acc / total_Iteration\n",
    "    print(f'test_acc: {test_acc}, takes {time.time()-start:.2f} secs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc: 0.5522835850715637, takes 3.22 secs\n"
     ]
    }
   ],
   "source": [
    "test(model, test_loader, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>**< 5가지 > 기본적으로 가르쳐 준 것들**\n",
    "    1. activation function -> relu  \n",
    "    2. weight initialization -> he  \n",
    "    3. batch normalization -> batchnorm1d  \n",
    "    4. optimizer -> adam  \n",
    "    5. dropout -> 정확도가 낮으므로 생략\n",
    "\n",
    "**< 그 외 추가가능 한 것들 >**\n",
    "     - data noramlization -> 데이터의 평균, 표준편차 구해서 데이터 자체를 정규화시킴\n",
    "     - data augmentation -> cnn때 추가, 여기선 생략. 어차피 데이터 조합적인 특징 못 뽑아냄\n",
    "     - lr_scheduler -> cnn때 추가\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
