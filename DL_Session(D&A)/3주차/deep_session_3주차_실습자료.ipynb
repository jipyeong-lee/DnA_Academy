{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Algorithm with MNIST (MLP model)\n",
    "\n",
    "<font color = \"#CC3D3D\"><p>\n",
    "- [Module Import](#Module-Import)\n",
    "- [Device Checking](#Device-Checking)\n",
    "- [Data Downloading](#Data-Downloading)\n",
    "- [Build Model](#Build-Model)\n",
    "- [Model Train, Test](#Model-Train,-Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torchvision import transforms, datasets\n",
    "import torch.nn.init as init\n",
    "import random "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Checking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version:  1.8.0 DEVICE: cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "print('Using PyTorch version: ', torch.__version__, 'DEVICE:', DEVICE)\n",
    "if DEVICE == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Downloading "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32  # MNIST train dataset - 60000, batch_size - 32, batch_num - 1875(60000/32)\n",
    "EPOCHS = 10\n",
    "Learning_Rate = 0.01  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(root = \"../data/MNIST\",\n",
    "                               train = True,   \n",
    "                               download = True,\n",
    "                               transform = transforms.ToTensor())\n",
    "\n",
    "test_dataset = datasets.MNIST(root = \"../data/MNIST\",\n",
    "                              train = False, \n",
    "                              transform = transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                           batch_size = BATCH_SIZE,\n",
    "                                           shuffle = True) \n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                          batch_size = BATCH_SIZE,\n",
    "                                          shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([32, 1, 28, 28]) type: torch.FloatTensor\n",
      "y_train: torch.Size([32]) type: torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "for (X_train, y_train) in train_loader:\n",
    "    print('X_train:', X_train.size(), 'type:', X_train.type())\n",
    "    print('y_train:', y_train.size(), 'type:', y_train.type())  # [Mini-Batch, Channel, Height, Weight]  \n",
    "    break\n",
    "# 32개의 이미지가 1개의 Mini-Batch를 구성하고 있고 가로 28개, 세로 28개의 픽셀로 구성돼 있으며 \n",
    "# 채녈이 1이므로 Gray Scale(흑백이미지)로 이루어진 이미지 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 실제 MNIST 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x72 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pltsize = 1\n",
    "plt.figure(figsize=(10 * pltsize, pltsize))\n",
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(X_train[i, :, :, :].numpy().reshape(28, 28), cmap = \"gray_r\")  # [32, 1, 28, 28] -> [28, 28, 1]\n",
    "    plt.title('Class: ' + str(y_train[i].item()))  # 이미지별로 라벨 할당"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build_basic_Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 딥러닝 모델 함수를 포함하는 nn.Module 클래스를 상속받는 Net클래스\n",
    "class Net(nn.Module):\n",
    "    # Net 클래스의 인스턴스를 생성했을 때 지니게 되는 성질을 정의해주는 메서드\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)  # 0부터 9까지 총 10가지 클래스를 표현하기 위한 Label값을 원-핫 인코딩으로 표현\n",
    "    \n",
    "    # forward propagation 정의    \n",
    "    def forward(self, x):  \n",
    "        x = x.view(-1, 28 * 28)  # MLP 모델은 1차원의 벡터 값을 입력으로 받아야 함\n",
    "        x = self.fc1(x)\n",
    "        x = F.sigmoid(x) # 활성화 함수 \n",
    "    \n",
    "        x = self.fc2(x)\n",
    "        x = F.sigmoid(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = F.log_softmax(x, dim = 1)  # 10가지 경우의 \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Net().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = Learning_Rate) # 옵티마이저 \n",
    "criterion = nn.CrossEntropyLoss() # 비용함수 \n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Train, Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, log_interval):\n",
    "    model.train()            # train을 하겠다고 선언해준다. \n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch_idx, (image, label) in enumerate(train_loader):\n",
    "        image = image.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(image)\n",
    "        loss = criterion(output, label)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tTrain Loss: {:.6f}\".format(\n",
    "                epoch, batch_idx * len(image), \n",
    "                len(train_loader.dataset), 100. * batch_idx / len(train_loader), \n",
    "                loss.item()))\n",
    "            \n",
    "    train_loss /= batch_idx\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    model.eval()                      # 평가를 한다고 선언해준다. \n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        for image, label in test_loader:\n",
    "            image = image.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "            output = model(image)\n",
    "            test_loss += criterion(output, label).item()\n",
    "            prediction = output.max(1, keepdim = True)[1]\n",
    "            correct += prediction.eq(label.view_as(prediction)).sum().item()\n",
    "            \n",
    "            \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 학습, 테스트 진행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leeji\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tTrain Loss: 2.366495\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tTrain Loss: 0.424147\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tTrain Loss: 0.267326\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tTrain Loss: 0.201989\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tTrain Loss: 0.194285\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tTrain Loss: 0.587600\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tTrain Loss: 0.292032\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tTrain Loss: 0.016331\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tTrain Loss: 0.121224\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tTrain Loss: 0.194711\n",
      "\n",
      "[EPOCH: 1], \tTest Loss: 0.0044, \tTest Accuracy: 95.82 % \n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tTrain Loss: 0.076636\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tTrain Loss: 0.146435\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tTrain Loss: 0.071511\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tTrain Loss: 0.083422\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tTrain Loss: 0.249258\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tTrain Loss: 0.112511\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tTrain Loss: 0.273832\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tTrain Loss: 0.127054\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tTrain Loss: 0.050195\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tTrain Loss: 0.049234\n",
      "\n",
      "[EPOCH: 2], \tTest Loss: 0.0037, \tTest Accuracy: 96.31 % \n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tTrain Loss: 0.122103\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tTrain Loss: 0.115888\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tTrain Loss: 0.117026\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tTrain Loss: 0.045378\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tTrain Loss: 0.060899\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tTrain Loss: 0.239557\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tTrain Loss: 0.189137\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tTrain Loss: 0.007420\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tTrain Loss: 0.093436\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tTrain Loss: 0.099334\n",
      "\n",
      "[EPOCH: 3], \tTest Loss: 0.0042, \tTest Accuracy: 96.08 % \n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tTrain Loss: 0.076152\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tTrain Loss: 0.060046\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tTrain Loss: 0.004595\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tTrain Loss: 0.008912\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tTrain Loss: 0.079853\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tTrain Loss: 0.272788\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tTrain Loss: 0.009442\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tTrain Loss: 0.287628\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tTrain Loss: 0.021688\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tTrain Loss: 0.009250\n",
      "\n",
      "[EPOCH: 4], \tTest Loss: 0.0042, \tTest Accuracy: 96.46 % \n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tTrain Loss: 0.095165\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tTrain Loss: 0.055174\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tTrain Loss: 0.023142\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tTrain Loss: 0.007925\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tTrain Loss: 0.026309\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tTrain Loss: 0.219473\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tTrain Loss: 0.013301\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tTrain Loss: 0.102734\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tTrain Loss: 0.066635\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tTrain Loss: 0.019262\n",
      "\n",
      "[EPOCH: 5], \tTest Loss: 0.0045, \tTest Accuracy: 96.08 % \n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tTrain Loss: 0.021022\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tTrain Loss: 0.017119\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tTrain Loss: 0.246631\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tTrain Loss: 0.014114\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tTrain Loss: 0.115559\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tTrain Loss: 0.059801\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tTrain Loss: 0.002213\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tTrain Loss: 0.391186\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tTrain Loss: 0.263106\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tTrain Loss: 0.309560\n",
      "\n",
      "[EPOCH: 6], \tTest Loss: 0.0042, \tTest Accuracy: 96.39 % \n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tTrain Loss: 0.153656\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tTrain Loss: 0.209258\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tTrain Loss: 0.003927\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tTrain Loss: 0.002469\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tTrain Loss: 0.220773\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tTrain Loss: 0.002687\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tTrain Loss: 0.027555\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tTrain Loss: 0.012587\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tTrain Loss: 0.226593\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tTrain Loss: 0.305784\n",
      "\n",
      "[EPOCH: 7], \tTest Loss: 0.0045, \tTest Accuracy: 96.34 % \n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tTrain Loss: 0.147227\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tTrain Loss: 0.003201\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tTrain Loss: 0.006645\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tTrain Loss: 0.005481\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tTrain Loss: 0.082638\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tTrain Loss: 0.019265\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tTrain Loss: 0.026530\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tTrain Loss: 0.154845\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tTrain Loss: 0.019754\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tTrain Loss: 0.031416\n",
      "\n",
      "[EPOCH: 8], \tTest Loss: 0.0042, \tTest Accuracy: 96.39 % \n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tTrain Loss: 0.114849\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tTrain Loss: 0.002554\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tTrain Loss: 0.217886\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tTrain Loss: 0.043386\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tTrain Loss: 0.087350\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tTrain Loss: 0.195011\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tTrain Loss: 0.030125\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tTrain Loss: 0.107984\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tTrain Loss: 0.211900\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tTrain Loss: 0.362592\n",
      "\n",
      "[EPOCH: 9], \tTest Loss: 0.0042, \tTest Accuracy: 96.68 % \n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tTrain Loss: 0.031374\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tTrain Loss: 0.046974\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tTrain Loss: 0.006743\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tTrain Loss: 0.337217\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tTrain Loss: 0.051099\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tTrain Loss: 0.403049\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tTrain Loss: 0.017334\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tTrain Loss: 0.080887\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tTrain Loss: 0.215481\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tTrain Loss: 0.137669\n",
      "\n",
      "[EPOCH: 10], \tTest Loss: 0.0047, \tTest Accuracy: 96.22 % \n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_loss_item = []\n",
    "test_loss_item = []\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss = train(model, train_loader, optimizer, log_interval = 200)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    train_loss_item.append(train_loss)\n",
    "    test_loss_item.append(test_loss)\n",
    "    print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(\n",
    "        epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build_algorithm_Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### apply - BatchNorm, DropOut, ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_al(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_al, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 512)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(512)                      # batch norm을 사용할 때 size를 맞춰주어야 한다.\n",
    "        \n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(256) \n",
    "        \n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "        \n",
    "        self.dropout_prob = 0.5                                     # 0.5 비율의 node수를 dropout 하여 학습한다.\n",
    "         \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)                                    # view 함수를 통해 28 * 28 size를 일렬로 펼쳐준다.\n",
    "        x = self.fc1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = F.relu(x) \n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)   # p 값에 dropout_prob가 들어간다.\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = F.log_softmax(x, dim = 1)    # MNIST data의 차원이 1차원이므로 dim=1로 설정한다.\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 같은 코드이지만 다른 스타일로 nn.Sequential을 사용하여 짤 수 있다.\n",
    "\n",
    "drop_prob = 0.5           # drop_prob는 hyper parameter이므로 미리 설정한다.\n",
    "class Net_al(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_al, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p = drop_prob)\n",
    "        )\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p = drop_prob)\n",
    "        )\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Linear(256, 10),\n",
    "            nn.LogSoftmax(dim = 1)\n",
    "        )\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### apply - He Initialization, Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_al(\n",
      "  (layer1): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=10, bias=True)\n",
      "    (1): LogSoftmax(dim=1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# He initialization\n",
    "def weight_init(m):\n",
    "    if isinstance(m, nn.Linear):                          # m이 nn.Linear이라면 실행해줌\n",
    "        init.kaiming_uniform_(m.weight.data)            # HE initialization을 적용하는 코드\n",
    "\n",
    "model_al = Net_al().to(DEVICE)\n",
    "model_al.apply(weight_init)                                                 # apply 함수를 통해 model에 가중치 초기화함수 적용\n",
    "optimizer = torch.optim.Adam(model_al.parameters(), lr = Learning_Rate)    # 옵티마이저 \n",
    "criterion = nn.CrossEntropyLoss()                                          # 비용함수 \n",
    "\n",
    "print(model_al)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 알고리즘이 있는 model 학습, 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tTrain Loss: 2.959459\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tTrain Loss: 0.824352\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tTrain Loss: 0.620122\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tTrain Loss: 0.217175\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tTrain Loss: 0.401572\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tTrain Loss: 0.070856\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tTrain Loss: 0.457302\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tTrain Loss: 0.359630\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tTrain Loss: 0.251777\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tTrain Loss: 0.165015\n",
      "\n",
      "[EPOCH: 1], \tTest Loss: 0.0039, \tTest Accuracy: 96.16 % \n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tTrain Loss: 0.123113\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tTrain Loss: 0.470857\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tTrain Loss: 0.271360\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tTrain Loss: 0.115568\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tTrain Loss: 0.185269\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tTrain Loss: 0.293502\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tTrain Loss: 0.073592\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tTrain Loss: 0.211666\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tTrain Loss: 0.103565\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tTrain Loss: 0.281128\n",
      "\n",
      "[EPOCH: 2], \tTest Loss: 0.0036, \tTest Accuracy: 96.38 % \n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tTrain Loss: 0.267971\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tTrain Loss: 0.195678\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tTrain Loss: 0.016857\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tTrain Loss: 0.041961\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tTrain Loss: 0.170759\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tTrain Loss: 0.152905\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tTrain Loss: 0.309236\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tTrain Loss: 0.253937\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tTrain Loss: 0.217773\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tTrain Loss: 0.238247\n",
      "\n",
      "[EPOCH: 3], \tTest Loss: 0.0031, \tTest Accuracy: 96.86 % \n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tTrain Loss: 0.187337\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tTrain Loss: 0.105887\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tTrain Loss: 0.190304\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tTrain Loss: 0.213297\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tTrain Loss: 0.045441\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tTrain Loss: 0.782667\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tTrain Loss: 0.169141\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tTrain Loss: 0.233971\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tTrain Loss: 0.215921\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tTrain Loss: 0.120992\n",
      "\n",
      "[EPOCH: 4], \tTest Loss: 0.0025, \tTest Accuracy: 97.56 % \n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tTrain Loss: 0.167760\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tTrain Loss: 0.068966\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tTrain Loss: 0.203758\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tTrain Loss: 0.070280\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tTrain Loss: 0.038194\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tTrain Loss: 0.533818\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tTrain Loss: 0.085806\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tTrain Loss: 0.021282\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tTrain Loss: 0.142276\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tTrain Loss: 0.037668\n",
      "\n",
      "[EPOCH: 5], \tTest Loss: 0.0025, \tTest Accuracy: 97.44 % \n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tTrain Loss: 0.280713\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tTrain Loss: 0.170475\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tTrain Loss: 0.398704\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tTrain Loss: 0.061746\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tTrain Loss: 0.097787\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tTrain Loss: 0.475778\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tTrain Loss: 0.087464\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tTrain Loss: 0.262430\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tTrain Loss: 0.069906\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tTrain Loss: 0.443448\n",
      "\n",
      "[EPOCH: 6], \tTest Loss: 0.0024, \tTest Accuracy: 97.75 % \n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tTrain Loss: 0.334883\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tTrain Loss: 0.299627\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tTrain Loss: 0.301951\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tTrain Loss: 0.027549\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tTrain Loss: 0.009567\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tTrain Loss: 0.179711\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tTrain Loss: 0.013543\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tTrain Loss: 0.005443\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tTrain Loss: 0.360150\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tTrain Loss: 0.446622\n",
      "\n",
      "[EPOCH: 7], \tTest Loss: 0.0022, \tTest Accuracy: 97.93 % \n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tTrain Loss: 0.456327\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tTrain Loss: 0.127227\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tTrain Loss: 0.035274\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tTrain Loss: 0.291076\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tTrain Loss: 0.168974\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tTrain Loss: 0.335553\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tTrain Loss: 0.104532\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tTrain Loss: 0.117390\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tTrain Loss: 0.446576\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tTrain Loss: 0.015340\n",
      "\n",
      "[EPOCH: 8], \tTest Loss: 0.0023, \tTest Accuracy: 97.69 % \n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tTrain Loss: 0.320887\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tTrain Loss: 0.040684\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tTrain Loss: 0.141286\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tTrain Loss: 0.107659\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tTrain Loss: 0.060684\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tTrain Loss: 0.395570\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tTrain Loss: 0.135382\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tTrain Loss: 0.163129\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tTrain Loss: 0.235813\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tTrain Loss: 0.173081\n",
      "\n",
      "[EPOCH: 9], \tTest Loss: 0.0022, \tTest Accuracy: 97.93 % \n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tTrain Loss: 0.129869\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tTrain Loss: 0.075919\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tTrain Loss: 0.348696\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tTrain Loss: 0.032162\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tTrain Loss: 0.195952\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tTrain Loss: 0.214186\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tTrain Loss: 0.132879\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tTrain Loss: 0.132689\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tTrain Loss: 0.202362\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tTrain Loss: 0.110757\n",
      "\n",
      "[EPOCH: 10], \tTest Loss: 0.0020, \tTest Accuracy: 97.97 % \n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_loss_item_al = []\n",
    "test_loss_item_al = []\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss = train(model_al, train_loader, optimizer, log_interval = 200)\n",
    "    test_loss, test_accuracy = evaluate(model_al, test_loader)\n",
    "    train_loss_item_al.append(train_loss)\n",
    "    test_loss_item_al.append(test_loss)\n",
    "    print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(\n",
    "        epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 두 model 비교해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = \"#CC3D3D\"><p>\n",
    "-[basic model 학습 결과](#학습,-테스트-진행) \n",
    "    \n",
    "-[알고리즘을 적용한 model 학습 결과](#알고리즘이-있는-model-학습,-테스트)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
