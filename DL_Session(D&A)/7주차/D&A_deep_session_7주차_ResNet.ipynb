{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Session 7주차\n",
    "## ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 1.8.1  Device: cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "\n",
    "print('Using PyTorch version:', torch.__version__, ' Device:', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../data/CIFAR_10\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9bdcd7828f04ef4969758c2aaaba7d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=170498071.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting ../data/CIFAR_10\\cifar-10-python.tar.gz to ../data/CIFAR_10\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.CIFAR10(root = \"../data/CIFAR_10\",\n",
    "                                  train = True,\n",
    "                                  download = True,\n",
    "                                  transform = transforms.Compose([\n",
    "                                    transforms.RandomHorizontalFlip(),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]))\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root = \"../data/CIFAR_10\",\n",
    "                                train = False,\n",
    "                                transform = transforms.Compose([\n",
    "                                    transforms.RandomHorizontalFlip(),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                            batch_size = BATCH_SIZE,\n",
    "                                            shuffle = True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                          batch_size = BATCH_SIZE,\n",
    "                                          shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#반복적으로 이용하는 block 정의\n",
    "class BasicBlock(nn.Module): #nn.Module 상속받아 정의\n",
    "    def __init__(self, in_planes, planes, stride = 1): #in_planes는 input으로 이용되는 데이터의 채널 수, planes는 필터의 갯수\n",
    "        super(BasicBlock, self).__init__() #nn.Module의 메서드를 상속받습니다.\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size = 3, stride = stride, padding = 1, bias = False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes) #input의 분포가 달라짐에 따라 학습속도가 느려지는 것을 막기 위해 사용\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size = 3, stride = 1, padding = 1, bias = False) #위에서 통과한 결과값 사용\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        \n",
    "        self.shortcut = nn.Sequential() #ResNet의 shortcut 정의\n",
    "        if stride != 1 or in_planes != planes: #두번째 블럭부터 적용되는 shorcut\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, kernel_size = 1, stride = stride, bias = False),\n",
    "                nn.BatchNorm2d(planes))\n",
    "    \n",
    "    def forward(self, x): #계산과정\n",
    "        out = F.relu(self.bn1(self.conv1(x))) #conv연산과 Batch Nomarlization 연산을 진행하고 ReLU를 적용\n",
    "        out = self.bn2(self.conv2(out)) #위에서 나온 값에 두번째 conv, BN을 적용\n",
    "        out += self.shortcut(x) #결과값과 shorcut을 통과한 결과값을 더함\n",
    "        out = F.relu(out) #shorcut까지 적용한 out에 relu적용\n",
    "        return out\n",
    "        \n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_classes = 10): #클래스 갯수 10개로 고정\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 16 #in_planes값을 16으로 고정\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size = 3, stride = 1, padding = 1, bias = False) #Basic Block의 conv1과 다름\n",
    "        #input으로 이용하는 컬러 이미지에 적용하는 필터\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(16, 2, stride = 1)\n",
    "        self.layer2 = self._make_layer(32, 2, stride = 2)\n",
    "        self.layer3 = self._make_layer(64, 2, stride = 2)\n",
    "        self.linear = nn.Linear(64, num_classes)\n",
    "        \n",
    "    def _make_layer(self, planes, num_blocks, stride): #여러 층의 레이어를 구성해 반환\n",
    "        strides = [stride] + [1] * (num_blocks  - 1) #stride 범위를 BasicBlock마다 설정할 수 있도록 정의\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(BasicBlock(self.in_planes, planes, stride)) #BasicBlock의 결과값을 추가\n",
    "            self.in_planes = planes #in_planes를 plane로 업데이트(shorcut을 계산하기 위해)\n",
    "        return nn.Sequential(*layers) #여러 층으로 생성한 레이어를 Sequential로 정의해 반환\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out) #input 16채널, output 16채널인 BasicBlock 2개\n",
    "        out = self.layer2(out) #input 16채널, output 32채널인 BasicBlock 1개 #input 32채널, output 32채널인 BasicBlock 1개\n",
    "        out = self.layer3(out) #input 32채널, output 64채널인 BasicBlock 1개 #input 64채널, output 64채널인 BasicBlock 1개\n",
    "        out = F.avg_pool2d(out, 8) #다운샘플링\n",
    "        out = out.view(out.size(0), -1) #일차원 벡터로 펼치기\n",
    "        out = self.linear(out) #10개의 노드로 구성된 FC와 연결해 10 크기의 벡터 출력 #CIFAR-10 의 원핫 인코딩과 비교해 loss를 계산\n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = ResNet().to(DEVICE)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, log_interval):\n",
    "    model.train()\n",
    "    for batch_idx, (image, label) in enumerate(train_loader):\n",
    "        image = image.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(image)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tTrain Loss: {:.6f}\".format(\n",
    "                epoch, batch_idx * len(image), \n",
    "                len(train_loader.dataset), 100. * batch_idx / len(train_loader), \n",
    "                loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, label in test_loader:\n",
    "            image = image.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "            output = model(image)\n",
    "            test_loss += criterion(output, label).item()\n",
    "            prediction = output.max(1, keepdim = True)[1]\n",
    "            correct += prediction.eq(label.view_as(prediction)).sum().item()\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/50000 (0%)]\tTrain Loss: 2.533777\n",
      "Train Epoch: 1 [6400/50000 (13%)]\tTrain Loss: 1.931172\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tTrain Loss: 1.537740\n",
      "Train Epoch: 1 [19200/50000 (38%)]\tTrain Loss: 1.964626\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tTrain Loss: 1.247007\n",
      "Train Epoch: 1 [32000/50000 (64%)]\tTrain Loss: 1.348087\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tTrain Loss: 1.291493\n",
      "Train Epoch: 1 [44800/50000 (90%)]\tTrain Loss: 1.084874\n",
      "\n",
      "[EPOCH: 1], \tTest Loss: 0.0403, \tTest Accuracy: 54.32 % \n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tTrain Loss: 1.212604\n",
      "Train Epoch: 2 [6400/50000 (13%)]\tTrain Loss: 1.159376\n",
      "Train Epoch: 2 [12800/50000 (26%)]\tTrain Loss: 1.023008\n",
      "Train Epoch: 2 [19200/50000 (38%)]\tTrain Loss: 0.972971\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tTrain Loss: 0.817964\n",
      "Train Epoch: 2 [32000/50000 (64%)]\tTrain Loss: 0.933632\n",
      "Train Epoch: 2 [38400/50000 (77%)]\tTrain Loss: 0.806798\n",
      "Train Epoch: 2 [44800/50000 (90%)]\tTrain Loss: 0.914227\n",
      "\n",
      "[EPOCH: 2], \tTest Loss: 0.0352, \tTest Accuracy: 61.86 % \n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tTrain Loss: 0.993993\n",
      "Train Epoch: 3 [6400/50000 (13%)]\tTrain Loss: 0.987090\n",
      "Train Epoch: 3 [12800/50000 (26%)]\tTrain Loss: 0.958426\n",
      "Train Epoch: 3 [19200/50000 (38%)]\tTrain Loss: 0.946413\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tTrain Loss: 0.712270\n",
      "Train Epoch: 3 [32000/50000 (64%)]\tTrain Loss: 0.951436\n",
      "Train Epoch: 3 [38400/50000 (77%)]\tTrain Loss: 0.663243\n",
      "Train Epoch: 3 [44800/50000 (90%)]\tTrain Loss: 0.867849\n",
      "\n",
      "[EPOCH: 3], \tTest Loss: 0.0266, \tTest Accuracy: 69.74 % \n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tTrain Loss: 0.903424\n",
      "Train Epoch: 4 [6400/50000 (13%)]\tTrain Loss: 1.045050\n",
      "Train Epoch: 4 [12800/50000 (26%)]\tTrain Loss: 0.946637\n",
      "Train Epoch: 4 [19200/50000 (38%)]\tTrain Loss: 0.557617\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tTrain Loss: 0.634275\n",
      "Train Epoch: 4 [32000/50000 (64%)]\tTrain Loss: 0.957383\n",
      "Train Epoch: 4 [38400/50000 (77%)]\tTrain Loss: 0.950401\n",
      "Train Epoch: 4 [44800/50000 (90%)]\tTrain Loss: 0.784222\n",
      "\n",
      "[EPOCH: 4], \tTest Loss: 0.0278, \tTest Accuracy: 68.79 % \n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tTrain Loss: 0.999256\n",
      "Train Epoch: 5 [6400/50000 (13%)]\tTrain Loss: 0.913603\n",
      "Train Epoch: 5 [12800/50000 (26%)]\tTrain Loss: 0.867105\n",
      "Train Epoch: 5 [19200/50000 (38%)]\tTrain Loss: 0.481338\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tTrain Loss: 0.592355\n",
      "Train Epoch: 5 [32000/50000 (64%)]\tTrain Loss: 0.531535\n",
      "Train Epoch: 5 [38400/50000 (77%)]\tTrain Loss: 0.411197\n",
      "Train Epoch: 5 [44800/50000 (90%)]\tTrain Loss: 0.561087\n",
      "\n",
      "[EPOCH: 5], \tTest Loss: 0.0230, \tTest Accuracy: 74.63 % \n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tTrain Loss: 0.675759\n",
      "Train Epoch: 6 [6400/50000 (13%)]\tTrain Loss: 0.389278\n",
      "Train Epoch: 6 [12800/50000 (26%)]\tTrain Loss: 0.735609\n",
      "Train Epoch: 6 [19200/50000 (38%)]\tTrain Loss: 0.625083\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tTrain Loss: 0.671989\n",
      "Train Epoch: 6 [32000/50000 (64%)]\tTrain Loss: 0.485457\n",
      "Train Epoch: 6 [38400/50000 (77%)]\tTrain Loss: 0.503174\n",
      "Train Epoch: 6 [44800/50000 (90%)]\tTrain Loss: 0.551855\n",
      "\n",
      "[EPOCH: 6], \tTest Loss: 0.0216, \tTest Accuracy: 76.10 % \n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tTrain Loss: 0.793627\n",
      "Train Epoch: 7 [6400/50000 (13%)]\tTrain Loss: 0.784725\n",
      "Train Epoch: 7 [12800/50000 (26%)]\tTrain Loss: 0.814633\n",
      "Train Epoch: 7 [19200/50000 (38%)]\tTrain Loss: 0.652695\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tTrain Loss: 0.506065\n",
      "Train Epoch: 7 [32000/50000 (64%)]\tTrain Loss: 0.476840\n",
      "Train Epoch: 7 [38400/50000 (77%)]\tTrain Loss: 0.618595\n",
      "Train Epoch: 7 [44800/50000 (90%)]\tTrain Loss: 0.507062\n",
      "\n",
      "[EPOCH: 7], \tTest Loss: 0.0206, \tTest Accuracy: 77.52 % \n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tTrain Loss: 0.460891\n",
      "Train Epoch: 8 [6400/50000 (13%)]\tTrain Loss: 0.271605\n",
      "Train Epoch: 8 [12800/50000 (26%)]\tTrain Loss: 0.410325\n",
      "Train Epoch: 8 [19200/50000 (38%)]\tTrain Loss: 0.456798\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tTrain Loss: 0.447909\n",
      "Train Epoch: 8 [32000/50000 (64%)]\tTrain Loss: 0.472874\n",
      "Train Epoch: 8 [38400/50000 (77%)]\tTrain Loss: 0.467415\n",
      "Train Epoch: 8 [44800/50000 (90%)]\tTrain Loss: 0.543954\n",
      "\n",
      "[EPOCH: 8], \tTest Loss: 0.0194, \tTest Accuracy: 79.08 % \n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tTrain Loss: 0.652856\n",
      "Train Epoch: 9 [6400/50000 (13%)]\tTrain Loss: 0.292109\n",
      "Train Epoch: 9 [12800/50000 (26%)]\tTrain Loss: 0.678804\n",
      "Train Epoch: 9 [19200/50000 (38%)]\tTrain Loss: 0.577203\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tTrain Loss: 0.663400\n",
      "Train Epoch: 9 [32000/50000 (64%)]\tTrain Loss: 0.487598\n",
      "Train Epoch: 9 [38400/50000 (77%)]\tTrain Loss: 0.669524\n",
      "Train Epoch: 9 [44800/50000 (90%)]\tTrain Loss: 0.643001\n",
      "\n",
      "[EPOCH: 9], \tTest Loss: 0.0183, \tTest Accuracy: 79.97 % \n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tTrain Loss: 0.296151\n",
      "Train Epoch: 10 [6400/50000 (13%)]\tTrain Loss: 0.323111\n",
      "Train Epoch: 10 [12800/50000 (26%)]\tTrain Loss: 0.377337\n",
      "Train Epoch: 10 [19200/50000 (38%)]\tTrain Loss: 0.639196\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tTrain Loss: 0.162518\n",
      "Train Epoch: 10 [32000/50000 (64%)]\tTrain Loss: 0.308531\n",
      "Train Epoch: 10 [38400/50000 (77%)]\tTrain Loss: 0.411733\n",
      "Train Epoch: 10 [44800/50000 (90%)]\tTrain Loss: 0.373970\n",
      "\n",
      "[EPOCH: 10], \tTest Loss: 0.0173, \tTest Accuracy: 81.09 % \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, train_loader, optimizer, log_interval = 200)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(\n",
    "        epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
